---
title: "Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation"
collection: publications
permalink: /publication/2024-05-02-paper-title-learningefficient
excerpt: 'The weighted squared loss is a common component in several Collaborative Filtering (CF) algorithms for item recommendation, including the representative implicit Alternating Least Squares (iALS). Despite its widespread use, this loss function lacks a clear connection to ranking objectives such as Discounted Cumulative Gain (DCG), posing a fundamental challenge in explaining the exceptional ranking performance observed in these algorithms. In this work, we make a breakthrough by establishing a connection between squared loss and ranking metrics through a Taylor expansion of the DCG-consistent surrogate loss—softmax loss. We also discover a new surrogate squared loss function, namely Ranking-Generalizable Squared (RG$^2$) loss, and conduct thorough theoretical analyses on the DCG-consistency of the proposed loss function. Later, we present an example of utilizing the RG$^2$
 loss with Matrix Factorization (MF), coupled with a generalization upper bound and an ALS optimization algorithm that leverages closed-form solutions over all items. Experimental results over three public datasets demonstrate the effectiveness of the RG$^2$  loss, exhibiting ranking performance on par with, or even surpassing, the softmax loss while achieving faster convergence.' 
date: 2024-05-02
venue: The 41st International Conference on Machine Learning (ICML2024), accepted
citation: 'Yuanhao Pu, Xiaolong Chen, Xu Huang, Jin Chen, Defu Lian*, Enhong Chen. Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation. The 41st International Conference on Machine Learning (ICML 2024), accepted, Vienna, Jul 2024..'
---

The weighted squared loss is a common component in several Collaborative Filtering (CF) algorithms for item recommendation, including the representative implicit Alternating Least Squares (iALS). Despite its widespread use, this loss function lacks a clear connection to ranking objectives such as Discounted Cumulative Gain (DCG), posing a fundamental challenge in explaining the exceptional ranking performance observed in these algorithms. In this work, we make a breakthrough by establishing a connection between squared loss and ranking metrics through a Taylor expansion of the DCG-consistent surrogate loss—softmax loss. We also discover a new surrogate squared loss function, namely Ranking-Generalizable Squared (RG$^2$) loss, and conduct thorough theoretical analyses on the DCG-consistency of the proposed loss function. Later, we present an example of utilizing the RG$^2$
 loss with Matrix Factorization (MF), coupled with a generalization upper bound and an ALS optimization algorithm that leverages closed-form solutions over all items. Experimental results over three public datasets demonstrate the effectiveness of the RG$^2$
 loss, exhibiting ranking performance on par with, or even surpassing, the softmax loss while achieving faster convergence. [pdf](https://openreview.net/pdf?id=D5IRvFF1lN) [code](https://github.com/yuanhao53/RG2)
